x <- y
x <- "y"
knitr::opts_chunk$set(echo = TRUE)
##import articles
dir()
##import articles
dir("../Data")
##import articles
dir("../")
##import articles
dir("../Data-TAD")
data <- lnt_read("../Data-TAD") #
# rm(list=ls()) #empty environment
##load libraries
library(LexisNexisTools)
library(tidyverse)
library(stringi)
##import articles
dir("../Data-TAD") #identify where data is saved
data <- lnt_read("../Data-TAD") #
data <- lnt_read("../Data-TAD/*") #
##import articles
dir("../Data-TAD") #identify where data is saved
##import articles
dir("../Data-TAD") #identify where data is saved
save(data, file = "data.RData") #save LNToutput
##import articles
dir("../Data-TAD") #identify where data is saved
data <- lnt_read("../Data-TAD") #
length(unique(data@meta$ID)) #
save(data, file = "data.RData") #save LNToutput
data <- data[!grepl("doclist", data@meta$Source_File),] #drop doclists
###----------------------REMOVE ARTICLES WITH MISSING DATES
is.na(data@meta$Date)
###----------------------REMOVE ARTICLES WITH MISSING DATES
length(is.na(data@meta$Date))
###----------------------REMOVE ARTICLES WITH MISSING DATES
nrow(is.na(data@meta$Date))
data[duplicated(data@articles$Article), ] #check if there are direct duplicates
data <- data[!duplicated(data@articles$Article), ] #remove direct duplicates
nrow(data)
##calculate distance (takes several minutes) - converts into dtm, compares articles published on same day, and calculates overlap
duplicates_data <- lnt_similarity(LNToutput =  data, #this is the same as similarity measure from quanteda, if rel-dist = off
threshold = 0.95,
rel_dist = F) #no Levenstein distance calculated - much faster
unique(data@meta$Date)
is.na(data@meta$Date))
is.na(data@meta$Date)
length((is.na(data@meta$Date))
length(is.na(data@meta$Date)
length(is.na(data@meta$Date))
nrow(is.na(data@meta$Date))
data@meta$Date
##calculate distance (takes several minutes) - converts into dtm, compares articles published on same day, and calculates overlap
duplicates_data <- lnt_similarity(LNToutput =  data, #this is the same as similarity measure from quanteda, if rel-dist = off
threshold = 0.95,
rel_dist = F) #no Levenstein distance calculated - much faster
View(data)
#validate
head(duplicates_TOI_0.95[order(duplicates_TOI_0.95$Similarity), ], n = 1)
#validate
head(duplicates_data[order(duplicates_data$Similarity), ], n = 1)
View(duplicates_data)
duplicates_data_0.8 <- lnt_similarity(LNToutput =  data, #this is the same as similarity measure from quanteda, if rel-dist = off
threshold = 0.8,
rel_dist = F) #no Levenstein distance calculated - much faster
#validate
head(duplicates_data_0.8[order(duplicates_data$Similarity), ], n = 1)
View(duplicates_data_0.8)
duplicates_data_0.8[order(duplicates_data$Similarity), ][7]
duplicates_data_0.8[order(duplicates_data$Similarity), ][8]
duplicates_data_0.8[duplicates_data$Similarity, ][7]
duplicates_data_0.8[duplicates_data_0.8][7]
#validate
head(duplicates_data_0.8[order(duplicates_data_0.8$Similarity), ], n = 1)
View(duplicates_data_0.8)
duplicates_data_0.8[7]
duplicates_data_0.8[11]
data <- data[!data@meta$ID %in% duplicates_data_0.95$ID_duplicate, ] #remove duplicates
data <- data[!data@meta$ID %in% duplicates_data$ID_duplicate, ] #remove duplicates
nrow(data)
head(data@articles)
head(data@meta)
save(data, file = "data.RData") #save LNToutput
meta_articles <- lnt_convert(data, to = "data.frame") #keep only one df
meta_paragraphs <- lnt_convert(data, to = "data.frame")
knitr::opts_chunk$set(echo = TRUE)
# rm(list=ls()) #empty environment
##load libraries
library(LexisNexisTools)
library(tidyverse)
library(stringi)
##import articles
dir("../Data-TAD") #identify where data is saved
data <- lnt_read("../Data-TAD") #individual
length(unique(data@meta$ID)) #4800 documents
save(data, file = "data.RData") #save LNToutput
load(file = "data.RData")
nrow(is.na(data@meta$Date)) #check that there are no missing dates
data[duplicated(data@articles$Article), ] #check if there are direct duplicates (there are 52)
data <- data[!duplicated(data@articles$Article), ] #remove direct duplicates
nrow(is.na(data@meta$Date))
##calculate distance (takes several minutes) - converts into dtm, compares articles published on same day, and calculates overlap
duplicates_data <- lnt_similarity(LNToutput =  data, #this is the same as similarity measure from quanteda, if rel-dist = off
threshold = 0.95,
rel_dist = F) #no Levenstein distance calculated - much faster
head(meta_paragraphs)
meta_paragraphs <- lnt_convert(data, to = "data.frame", what = "Paragraphs")
###update ID variables
head(meta_paragraphs)
#new ID
meta_paragraphs$Art_ID <- with(meta_paragraphs, paste0(Art_ID, Newspaper, Headline, Date)) #new ID
#new ID
df$Art_ID <- with(meta_paragraphs, paste0(Art_ID, Newspaper, Headline, Date)) #new ID
#new ID
meta_paragraphs$Art_ID <- with(meta_paragraphs, paste0(Art_ID, Newspaper, Headline, Date)) #new ID
head(meta_paragraphs)
df <- meta_paragraphs
df <- data
data <- meta_paragraphs
unique(data$Newspaper)
#new ID
meta_paragraphs$Art_ID <- with(meta_paragraphs, paste0(Art_ID, Newspaper, Headline, Date)) #new ID
data <- meta_paragraphs
unique(data$Newspaper)
meta_paragraphs <- lnt_convert(data, to = "data.frame", what = "Paragraphs")
data <- df
meta_articles <- lnt_convert(data, to = "data.frame") #keep only one df
meta_paragraphs <- lnt_convert(data, to = "data.frame", what = "Paragraphs")
unique(meta_pargraphs$Newspaper)
unique(meta_paragrphs$Newspaper)
unique(meta_paragraphs$Newspaper)
data <- data[!data@meta$ID %in% duplicates_data$ID_duplicate, ] #remove duplicates
load(file = "data.RData")
nrow(is.na(data@meta$Date)) #check that there are no missing dates
data[duplicated(data@articles$Article), ] #check if there are direct duplicates (there are 52)
data <- data[!duplicated(data@articles$Article), ] #remove direct duplicates
data <- data[!data@meta$ID %in% duplicates_data$ID_duplicate, ] #remove duplicates
unique(data@meta$Date)
dates <- unique(data@meta$Date)
dates
max.print(dates)
unique(data@meta$Newspaper)
data@meta[8]
data@meta[][8]
data@meta[,][8]
data@meta[,8]
data@meta[8,]
unique(data@meta$Newspaper)
data@meta$Newspaper == "May 4, 2012"
data@meta[data@meta$Newspaper == "May 4, 2012",]
unique(data@meta$Newspaper)
data@meta[data@meta$Newspaper == "March 20, 2009", ]
unique(data@meta$Newspaper)
save(data, file = "data.RData") #save LNToutput
load(file = "data.RData")
unique(data@meta$Newspaper)
data@meta <- data@meta %>%
data@meta <- data@meta %>%
mutate(Newspaper = recode(Newspaper,
"The Guardian(London)" = "The Guardian",
"The Times (London)" = "The Times",
"Guardian.com" = "The Guardian",
"DAILY MAIL (London)" = "Daily Mail",
"The Guardian (London) - Final Edition" = "The Guardian",
"May 4, 2012" = "The Times",
"The Guardian - Final Edition" = "The Guardian",
"Mail on Sunday (London)" = "Daily Mail",
"MAIL ON SUNDAY (London)" = "Daily Mail",
"March 20, 2009" = "The Times",
"MAIL ON SUNDAY" = "Daily Mail",
"March 27, 2009" = "The Times",
"Guardian.com." = "The Times",
"June 21, 2011" = "The Times",
"March 26, 2009"= "The Times",
"March 25, 2009" = "The Times",
"May 22, 2012" = "The Times",
"February 13, 2009" = "The Times",
"February 17, 2009" = "The Times",
"March 5, 2009" = "The Times",
"March 18, 2009" = "The Times",
"January 21, 2009" = "The Times",
"March 13, 2009" = "The Times",
"March 20, 2012" = "The Times",
"March 24, 2009" = "The Times",
"March 9, 2009" = "The Times",
"March 15, 2009" = "The Times",
"February 12, 2009" = "The Times",
"March 22, 2009"= "The Times",
"February 18, 2009" = "The Times",
"March 21, 2009" = "The Times",
"March 15, 2012" = "The Times",
"March 16, 2009" = "The Times",
"March 17, 2012" = "The Times",
"September 9, 2010" = "The Times",
"March 8, 2009" = "The Times",
"May 25, 2012" = "The Times",
"September 11, 2010" = "The Times",
"February 5, 2009" = "The Times",
"February 16, 2009" = "The Times",
"March 4, 2009" = "The Times",
"March 14, 2009" = "The Times",
"March 19, 2009" = "The Times",
"October 13, 2010" = "The Times",
"April 6, 2009" = "The Times"))
library(tidyverse)
data@meta <- data@meta %>%
mutate(Newspaper = recode(Newspaper,
"The Guardian(London)" = "The Guardian",
"The Times (London)" = "The Times",
"Guardian.com" = "The Guardian",
"DAILY MAIL (London)" = "Daily Mail",
"The Guardian (London) - Final Edition" = "The Guardian",
"May 4, 2012" = "The Times",
"The Guardian - Final Edition" = "The Guardian",
"Mail on Sunday (London)" = "Daily Mail",
"MAIL ON SUNDAY (London)" = "Daily Mail",
"March 20, 2009" = "The Times",
"MAIL ON SUNDAY" = "Daily Mail",
"March 27, 2009" = "The Times",
"Guardian.com." = "The Times",
"June 21, 2011" = "The Times",
"March 26, 2009"= "The Times",
"March 25, 2009" = "The Times",
"May 22, 2012" = "The Times",
"February 13, 2009" = "The Times",
"February 17, 2009" = "The Times",
"March 5, 2009" = "The Times",
"March 18, 2009" = "The Times",
"January 21, 2009" = "The Times",
"March 13, 2009" = "The Times",
"March 20, 2012" = "The Times",
"March 24, 2009" = "The Times",
"March 9, 2009" = "The Times",
"March 15, 2009" = "The Times",
"February 12, 2009" = "The Times",
"March 22, 2009"= "The Times",
"February 18, 2009" = "The Times",
"March 21, 2009" = "The Times",
"March 15, 2012" = "The Times",
"March 16, 2009" = "The Times",
"March 17, 2012" = "The Times",
"September 9, 2010" = "The Times",
"March 8, 2009" = "The Times",
"May 25, 2012" = "The Times",
"September 11, 2010" = "The Times",
"February 5, 2009" = "The Times",
"February 16, 2009" = "The Times",
"March 4, 2009" = "The Times",
"March 14, 2009" = "The Times",
"March 19, 2009" = "The Times",
"October 13, 2010" = "The Times",
"April 6, 2009" = "The Times"))
unique(data@meta$Newspaper)
data@meta <- data@meta %>%
mutate(Newspaper = recode(Newspaper,
"The Guardian(London)" = "The Guardian",
"The Times (London)" = "The Times",
"Daily Mail (London)" = "Daily Mail",
"The Guardian (London)" = "The Guardian",
"Guardian.com" = "The Guardian",
"DAILY MAIL (London)" = "Daily Mail",
"The Guardian (London) - Final Edition" = "The Guardian",
"May 4, 2012" = "The Times",
"The Guardian - Final Edition" = "The Guardian",
"Mail on Sunday (London)" = "Daily Mail",
"MAIL ON SUNDAY (London)" = "Daily Mail",
"March 20, 2009" = "The Times",
"MAIL ON SUNDAY" = "Daily Mail",
"March 27, 2009" = "The Times",
"Guardian.com." = "The Times",
"June 21, 2011" = "The Times",
"March 26, 2009"= "The Times",
"March 25, 2009" = "The Times",
"May 22, 2012" = "The Times",
"February 13, 2009" = "The Times",
"February 17, 2009" = "The Times",
"March 5, 2009" = "The Times",
"March 18, 2009" = "The Times",
"January 21, 2009" = "The Times",
"March 13, 2009" = "The Times",
"March 20, 2012" = "The Times",
"March 24, 2009" = "The Times",
"March 9, 2009" = "The Times",
"March 15, 2009" = "The Times",
"February 12, 2009" = "The Times",
"March 22, 2009"= "The Times",
"February 18, 2009" = "The Times",
"March 21, 2009" = "The Times",
"March 15, 2012" = "The Times",
"March 16, 2009" = "The Times",
"March 17, 2012" = "The Times",
"September 9, 2010" = "The Times",
"March 8, 2009" = "The Times",
"May 25, 2012" = "The Times",
"September 11, 2010" = "The Times",
"February 5, 2009" = "The Times",
"February 16, 2009" = "The Times",
"March 4, 2009" = "The Times",
"March 14, 2009" = "The Times",
"March 19, 2009" = "The Times",
"October 13, 2010" = "The Times",
"April 6, 2009" = "The Times"))
unique(data@meta$Newspaper)
data@meta <- data@meta %>%
mutate(Newspaper = recode(Newspaper,
"The Guardian(London)" = "The Guardian",
"The Times (London)" = "The Times",
"Daily Mail (London)" = "Daily Mail",
"The Guardian (London)" = "The Guardian",
"Guardian.com" = "The Guardian",
"DAILY MAIL (London)" = "Daily Mail",
"The Guardian (London) - Final Edition" = "The Guardian",
"May 4, 2012" = "The Times",
"The Guardian - Final Edition" = "The Guardian",
"Mail on Sunday (London)" = "Daily Mail",
"MAIL ON SUNDAY (London)" = "Daily Mail",
"March 20, 2009" = "The Times",
"MAIL ON SUNDAY" = "Daily Mail",
"March 27, 2009" = "The Times",
"Guardian.com." = "The Times",
"June 21, 2011" = "The Times",
"March 26, 2009"= "The Times",
"March 25, 2009" = "The Times",
"May 22, 2012" = "The Times",
"February 13, 2009" = "The Times",
"February 17, 2009" = "The Times",
"March 5, 2009" = "The Times",
"March 18, 2009" = "The Times",
"January 21, 2009" = "The Times",
"March 13, 2009" = "The Times",
"March 20, 2012" = "The Times",
"March 24, 2009" = "The Times",
"March 9, 2009" = "The Times",
"March 15, 2009" = "The Times",
"February 12, 2009" = "The Times",
"March 22, 2009"= "The Times",
"February 18, 2009" = "The Times",
"March 21, 2009" = "The Times",
"March 15, 2012" = "The Times",
"March 16, 2009" = "The Times",
"March 17, 2012" = "The Times",
"September 9, 2010" = "The Times",
"March 8, 2009" = "The Times",
"May 25, 2012" = "The Times",
"September 11, 2010" = "The Times",
"February 5, 2009" = "The Times",
"February 16, 2009" = "The Times",
"March 4, 2009" = "The Times",
"March 14, 2009" = "The Times",
"March 19, 2009" = "The Times",
"October 13, 2010" = "The Times",
"April 6, 2009" = "The Times"))
unique(data@meta$Newspaper)
meta_articles <- lnt_convert(data, to = "data.frame") #keep only one df
meta_paragraphs <- lnt_convert(data, to = "data.frame", what = "Paragraphs")
# #new ID
meta_paragraphs$Art_ID <- with(meta_paragraphs, paste0(Art_ID, Newspaper, Headline, Date)) #new ID
##Split Year variable
data <- data %>%
dplyr::mutate(Year = lubridate::year(Date),
Month = lubridate::month(Date),
Day = lubridate::day(Date))
##Split Year variable
data@meta <- data@meta %>%
dplyr::mutate(Year = lubridate::year(Date),
Month = lubridate::month(Date),
Day = lubridate::day(Date))
meta_articles <- lnt_convert(data, to = "data.frame") #keep only one df
meta_paragraphs <- lnt_convert(data, to = "data.frame", what = "Paragraphs")
head(data@paragraphs)
unique(data@paragraphs$Art_ID)
##Split Year variable
data@meta <- data@meta %>%
dplyr::mutate(Year = lubridate::year(Date),
Month = lubridate::month(Date),
Day = lubridate::day(Date))
head(data@paragraphs)
##drop unnecessary columns
data <- subset(data, select = -c(Edition, Graphic, Source_File))
# meta_articles <- lnt_convert(data, to = "data.frame") #keep only one df
meta_paragraphs <- lnt_convert(data, to = "data.frame", what = "Paragraphs")
# meta_articles <- lnt_convert(data, to = "data.frame") #keep only one df
#meta_paragraphs
mp <- lnt_convert(data, to = "data.frame", what = "Paragraphs")
##make dfs that include relevant paragraphs
lang <- mp[grep("Language:", mp$Paragraph), ]
##make dfs that include relevant paragraphs
lang <- mp[grep("Language:", mp$Paragraph), ]
publ <- mp[grep("Publication-Type:", mp$Paragraph), ]
publ <- mp[grep("Publication-Type:", mp$Paragraph), ]
classi <- mp[grep("Classification", mp$Paragraph), ]
classi <- mp[grep("Classification", mp$Paragraph), ]
subj <- mp[grep("Subject:", mp$Paragraph), ]
indust <- mp[grep("Industry:", mp$Paragraph), ]
geo <- mp[grep("Geographic:", mp$Paragraph), ]
geo <- mp[grep("Geographic:", mp$Paragraph), ]
pers <- mp[grep("Person:", mp$Paragraph), ]
pers <- mp[grep("Person:", mp$Paragraph), ]
comp <- mp[grep("Company:", mp$Paragraph), ]
tick <- mp[grep("Ticker:", mp$Paragraph), ]
##make vector including IDs of Paragraphs to be dropped
drop_par <- c(lang$Par_ID, publ$Par_ID, classi$Par_ID, subj$Par_ID, indust$Par_ID, geo$Par_ID, pers$Par_ID, comp$Par_ID, tick$Par_ID)
##remove these paragraphs from dataset
mp <- mp %>%
filter(!Par_ID %in% drop_par)
head(mp$Paragraph, n = 15)
journ <- mp[grep("Journal Code:", mp$Paragraph), ]
journ
journ <- mp[grep("Journal Code:", mp$Paragraph), ]
journ <- mp[grep("Journal Code:", mp$Paragraph), ]
##make vector including IDs of Paragraphs to be dropped
drop_par <- c(lang$Par_ID, publ$Par_ID, classi$Par_ID, subj$Par_ID, indust$Par_ID, geo$Par_ID, pers$Par_ID, comp$Par_ID, tick$Par_ID, journ$Par_ID)
##make vector including IDs of Paragraphs to be dropped
drop_par <- c(lang$Par_ID, publ$Par_ID, classi$Par_ID, subj$Par_ID, indust$Par_ID, geo$Par_ID, pers$Par_ID, comp$Par_ID, tick$Par_ID, journ$Par_ID)
##remove these paragraphs from dataset
mp <- mp %>%
filter(!Par_ID %in% drop_par)
head(mp$Paragraph, n = 20)
rm(drop_par)
head(mp$Paragraph, n = 20)
View(mp)
View(mp)
articles <- aggregate(Paragraph ~ Art_ID, data = mp, FUN = paste, collapse = "")
##remove paragraphs with less than three words
mp %>% filter(nchar(Paragraph) < 3)
View(mp)
##remove paragraphs with less than three words
mp <- mp %>% filter(nchar(Paragraph) < 3)
View(mp)
# meta_articles <- lnt_convert(data, to = "data.frame") #keep only one df
#meta_paragraphs
mp <- lnt_convert(data, to = "data.frame", what = "Paragraphs")
##-------------------do some basic data cleaning
# ##remove formatting issues: \\x95The land or \\x93Unchecked
# # step might become unnecessary if I remove pubctuation and numbers for analysis
# data$Paragraph <- stri_replace_all(data$Paragraph, "", regex = "\\\\x\\d\\d")
##--------------------extract subject classification info and remove irrelavnt infor paragraphs
##make dfs that include relevant paragraphs
lang <- mp[grep("Language:", mp$Paragraph), ]
publ <- mp[grep("Publication-Type:", mp$Paragraph), ]
classi <- mp[grep("Classification", mp$Paragraph), ]
subj <- mp[grep("Subject:", mp$Paragraph), ]
indust <- mp[grep("Industry:", mp$Paragraph), ]
geo <- mp[grep("Geographic:", mp$Paragraph), ]
pers <- mp[grep("Person:", mp$Paragraph), ]
comp <- mp[grep("Company:", mp$Paragraph), ]
tick <- mp[grep("Ticker:", mp$Paragraph), ]
journ <- mp[grep("Journal Code:", mp$Paragraph), ]
##make vector including IDs of Paragraphs to be dropped
drop_par <- c(lang$Par_ID, publ$Par_ID, classi$Par_ID, subj$Par_ID, indust$Par_ID, geo$Par_ID, pers$Par_ID, comp$Par_ID, tick$Par_ID, journ$Par_ID)
# ##remove irrelevant dataframes
# rm(lang, classi, tick)
##remove these paragraphs from dataset
mp <- mp %>%
filter(!Par_ID %in% drop_par)
head(mp$Paragraph, n = 20)
rm(drop_par)
##remove paragraphs with less than three words
mp1 <- mp %>% filter(nchar(Paragraph) > 2)
View(mp1)
stri_trim(mp$Paragraph)
mp1$Paragraph <- stri_trim(mp$Paragraph)
###remove empty paragraphs
mp1 <- mp
mp1$Paragraph <- stri_trim(mp$Paragraph)
View(mp1)
##remove paragraphs with less than three words
mp1 <- mp1 %>% filter(nchar(Paragraph) > 2)
View(mp1)
mp1$Paragraph <- stri_trim(mp$Paragraph)
mp1 <- mp1 %>%  str_detect(!(filter(Paragraph, '\\w*\\s\\w*\\s\\w*')))
mp1 <- str_detect(!(filter(mp1$Paragraph, '\\w*\\s\\w*\\s\\w*')))
mp1 <- str_detect(!(filter(mp1$Paragraph, pattern = '\\w*\\s\\w*\\s\\w*')))
?str_detect
mp1 <- filter(!(str_detect(mp1$Paragraph, pattern = '\\w*\\s\\w*\\s\\w*')))
mp1=mp1[!grepl('\\w*\\s\\w*\\s\\w*',mp1$Paragraph),]
View(mp1)
###remove empty paragraphs
mp1 <- mp
mp1<- mp1[grepl('\\w*\\s\\w*\\s\\w*',mp1$Paragraph),]
##remove paragraphs with less than three words
mp <- mp[grepl('\\w*\\s\\w*\\s\\w*',mp1$Paragraph),]
##remove paragraphs with less than three words
mp <- mp[grepl('\\w*\\s\\w*\\s\\w*',mp$Paragraph),]
