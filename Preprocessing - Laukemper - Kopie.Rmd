---
title: "Preprocessing - Thesis"
output: html_document
date: "2022-10-17"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##### Header
```{r}
# rm(list=ls()) #empty environment
##load libraries
library(LexisNexisTools)
library(tidyverse)
library(stringi)
```


```{r Import articles}
##import articles
dir("../Data-TAD") #identify where data is saved
data <- lnt_read("../Data-TAD") #individual
length(unique(data@meta$ID)) #4800 documents

save(data, file = "data.RData") #save LNToutput

```

###Load Corpi
```{r}
load(file = "data.RData")
```

###---------------------------Remove direct duplicates

```{r check for missing dates & rm direct duplicates}
nrow(is.na(data@meta$Date)) #check that there are no missing dates
data[duplicated(data@articles$Article), ] #check if there are direct duplicates (there are 52)
data <- data[!duplicated(data@articles$Article), ] #remove direct duplicates
```


###----------------------------Remove indirect duplicates

Why did I get there error meassage: You supplied NA values to dates, when actually no NAs

We manually compared the texts of potential duplicates with varius similarity scores. In the end, we decided on a cut-off of 0.95.

```{r}
nrow(is.na(data@meta$Date))
##calculate distance (takes several minutes) - converts into dtm, compares articles published on same day, and calculates overlap
duplicates_data <- lnt_similarity(LNToutput =  data, #this is the same as similarity measure from quanteda, if rel-dist = off
                                      threshold = 0.95,
                                      rel_dist = F) #no Levenstein distance calculated - much faster


#validate
head(duplicates_data[order(duplicates_data$Similarity), ], n = 1)
duplicates_data[11]
```

###REMOVE INDIRECT DUPLICATES (COMMENTED OUT)
```{r}
data <- data[!data@meta$ID %in% duplicates_data$ID_duplicate, ] #remove duplicates
nrow(data) #4617 articles left
head(data@meta)
```
###SAVE Dup removed data
```{r}
save(data, file = "data.RData") #save LNToutput
```


```{r}
load(file = "data.RData")
```


##-----------------------TURN INTO META_Para and META_article dfs
```{r}
data@meta[8,] 
data@meta[data@meta$Newspaper == "May 4, 2012",] #Times London

data@meta[data@meta$Newspaper == "March 20, 2009", ] #Times London, Times London, Times London




dates <- unique(data@meta$Date)
unique(data@meta$Newspaper)


```
```{r correct Newspaper}
library(tidyverse)
data@meta <- data@meta %>%
    mutate(Newspaper = recode(Newspaper,
"The Guardian(London)" = "The Guardian",            
"The Times (London)" = "The Times",          
"Daily Mail (London)" = "Daily Mail",
"The Guardian (London)" = "The Guardian",
"Guardian.com" = "The Guardian",                        
"DAILY MAIL (London)" = "Daily Mail",              
"The Guardian (London) - Final Edition" = "The Guardian",
"May 4, 2012" = "The Times",          
"The Guardian - Final Edition" = "The Guardian",
"Mail on Sunday (London)" = "Daily Mail",              
"MAIL ON SUNDAY (London)" = "Daily Mail",
"March 20, 2009" = "The Times",          
"MAIL ON SUNDAY" = "Daily Mail",                 
"March 27, 2009" = "The Times",                     
"Guardian.com." = "The Times",                       
"June 21, 2011" = "The Times",                       
"March 26, 2009"= "The Times",                       
"March 25, 2009" = "The Times",                      
"May 22, 2012" = "The Times",                        
"February 13, 2009" = "The Times",                 
"February 17, 2009" = "The Times",                   
"March 5, 2009" = "The Times",                       
"March 18, 2009" = "The Times",                       
"January 21, 2009" = "The Times",                     
"March 13, 2009" = "The Times",                       
"March 20, 2012" = "The Times",                       
"March 24, 2009" = "The Times",                      
"March 9, 2009" = "The Times",                        
"March 15, 2009" = "The Times",                       
"February 12, 2009" = "The Times",                    
"March 22, 2009"= "The Times",                  
"February 18, 2009" = "The Times",                    
"March 21, 2009" = "The Times",                      
"March 15, 2012" = "The Times",                      
"March 16, 2009" = "The Times",                       
"March 17, 2012" = "The Times",                       
"September 9, 2010" = "The Times",                    
"March 8, 2009" = "The Times",                       
"May 25, 2012" = "The Times",
"September 11, 2010" = "The Times",                   
"February 5, 2009" = "The Times",                     
"February 16, 2009" = "The Times",                    
"March 4, 2009" = "The Times",                        
"March 14, 2009" = "The Times",                       
"March 19, 2009" = "The Times",                       
"October 13, 2010" = "The Times",                     
"April 6, 2009" = "The Times"))

unique(data@meta$Newspaper)
```
##-------------------add missing variables
```{r}
data@meta$Date

##Split Year variable
data@meta <- data@meta %>%
  dplyr::mutate(Year = lubridate::year(Date), 
                Month = lubridate::month(Date), 
                Day = lubridate::day(Date))

head(data@paragraphs)

# ##make Paragraph ID
# data$Par_ID <- with(data, paste0(Par_ID, Newspaper, Headline, Year))

# ##drop unnecessary columns
# data <- subset(data, select = -c(Edition, Graphic, Source_File))
```

```{r}
# meta_articles <- lnt_convert(data, to = "data.frame") #keep only one df 
#meta_paragraphs
mp <- lnt_convert(data, to = "data.frame", what = "Paragraphs")
```

##----------------turn paragraphs into single dataframe
```{r}
###update ID variables
# meta_paragraphs$Art_ID <- with(meta_paragraphs, paste0(Art_ID, Newspaper, Headline, Date)) #new ID
# 
# data <- meta_paragraphs
```

##basic data cleaning (removing unnecessary sections)
```{r}
##-------------------do some basic data cleaning

# ##remove formatting issues: \\x95The land or \\x93Unchecked
# # step might become unnecessary if I remove pubctuation and numbers for analysis
# data$Paragraph <- stri_replace_all(data$Paragraph, "", regex = "\\\\x\\d\\d")

##--------------------extract subject classification info and remove irrelavnt infor paragraphs

##make dfs that include relevant paragraphs
lang <- mp[grep("Language:", mp$Paragraph), ]
publ <- mp[grep("Publication-Type:", mp$Paragraph), ]
classi <- mp[grep("Classification", mp$Paragraph), ]
subj <- mp[grep("Subject:", mp$Paragraph), ]
indust <- mp[grep("Industry:", mp$Paragraph), ]
geo <- mp[grep("Geographic:", mp$Paragraph), ]
pers <- mp[grep("Person:", mp$Paragraph), ]
comp <- mp[grep("Company:", mp$Paragraph), ]
tick <- mp[grep("Ticker:", mp$Paragraph), ]
journ <- mp[grep("Journal Code:", mp$Paragraph), ]

##make vector including IDs of Paragraphs to be dropped
drop_par <- c(lang$Par_ID, publ$Par_ID, classi$Par_ID, subj$Par_ID, indust$Par_ID, geo$Par_ID, pers$Par_ID, comp$Par_ID, tick$Par_ID, journ$Par_ID)

# ##remove irrelevant dataframes
# rm(lang, classi, tick)

##remove these paragraphs from dataset 
mp <- mp %>%
  filter(!Par_ID %in% drop_par)

head(mp$Paragraph, n = 20)
rm(drop_par)



```


```{r}

###remove empty paragraphs
mp1 <- mp

mp1$Paragraph <- stri_trim(mp$Paragraph)

##remove paragraphs with less than three words
mp1 <- mp1 %>% filter(nchar(Paragraph) > 2)





```

##----------------aggragate paragraphs into article text
#make df of aggregated paragraphs
#?can I use sth else in collapse, e.g. \n to allow quanteda to reshape from text to paragraph

```{r}
articles <- aggregate(Paragraph ~ Art_ID, data = mp, FUN = paste, collapse = "")

#rename Paragraph variable into Text
articles <- rename(articles, Text = Paragraph) #rename Paragraph value

#make dataset for merger with only one row per Art_ID
data2 <- subset(data, select = -c(Paragraph))
data2 = data2[!duplicated(data2$Art_ID),]

#merge dataset to get articles df
articles <- merge(articles, data2, by = "Art_ID")

rm(data2) #remove unnecessary df
para <- data #rename paragraphs df
rm (data)

head(articles$Text, n =1)

rm(comp, geo, inust, pers, publ, subj, subj2, indust)
```


```{r}
# Save multiple objects
save(articles, para, file = "Data/artpara.RData")
# To load the data again
load("Data/artpara.RData")

#save articles as csv
write.csv(articles,"Data/articles.csv", row.names = T)
```


